<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>分词</title>
    <url>/%E5%88%86%E8%AF%8D/</url>
    <content><![CDATA[<h1 id="1-分词技术"><a href="#1-分词技术" class="headerlink" title="1.分词技术"></a>1.分词技术</h1><h2 id="1-1基于词典的分词"><a href="#1-1基于词典的分词" class="headerlink" title="1.1基于词典的分词"></a>1.1基于词典的分词</h2><h3 id="1-1-2最大路径匹配"><a href="#1-1-2最大路径匹配" class="headerlink" title="1.1.2最大路径匹配"></a>1.1.2最大路径匹配</h3><p>用词典构成一颗trie树,即字典树，从根结点匹配trie树进行查找，直到不能再继续往下的时候形成一个词，可分为正向最大匹配和反向最大匹配，在hanlp中还会有双向最大匹配，双向最大匹配结合了正向和方向最大匹配，用两种方法切出来后选择切出的总词数最少的，在词数相同的时候选择反向最大匹配的结果。</p>
<h3 id="1-1-3-最短路径分词算法"><a href="#1-1-3-最短路径分词算法" class="headerlink" title="1.1.3 最短路径分词算法"></a>1.1.3 最短路径分词算法</h3><p>最短路径分词算法首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的最短路径作为最佳组合方式</p>
<p>求解最短路径的方法：贪心算法或动态规划两种求解算</p>
<p>1）Dijkstra算法求解最短路径：</p>
<p>该算法适用于所有带权有向图，求解源节点到其他所有节点的最短路径，并可以求得全局最优解。Dijkstra本质为贪心算法，在每一步走到当前路径最短的节点，递推地更新原节点到其他节点的距离。针对当前问题，Dijkstra算法的计算结果为：“他／说／的／确实／在理“。可见最短路径分词算法可以满足部分分词要求。但当存在多条距离相同的最短路径时，Dijkstra只保存一条，对其他路径不公平，也缺乏理论依据。</p>
<p>2） N-最短路径分词算法：</p>
<p>N-最短路径分词是对Dijkstra算法的扩展，在每一步保存最短的N条路径，并记录这些路径上当前节点的前驱，在最后求得最优解时回溯得到最短路径。该方法的准确率优于Dijkstra算法，但在时间和空间复杂度上都更大。</p>
<h2 id="1-1-4基于n-gram-model的分词算法"><a href="#1-1-4基于n-gram-model的分词算法" class="headerlink" title="1.1.4基于n-gram model的分词算法"></a>1.1.4基于n-gram model的分词算法</h2><p>求最大路径概率，将词的条件概率p(word_t|word_t-1)作为路径的权重，求路径的联合概率分布的最大概率。p(x1,x2,x3,,,,xn)=p(x1|</s>)<em>p(x2|x1)</em>….p(xn|xn-1)</p>
<h2 id="1-2基于字的分词"><a href="#1-2基于字的分词" class="headerlink" title="1.2基于字的分词"></a>1.2基于字的分词</h2><p>基于字的分词事先不对句子进行词的匹配，而是将分词看成序列标注问题，把一个字标记成B(Begin), I(Inside), O(Outside), E(End), S(Single)。因此也可以看成是每个字的分类问题，输入为每个字及其前后字所构成的特征，输出为分类标记。对于分类问题，可以用统计机器学习或神经网络的方法求解。</p>
<h3 id="1-2-1生成式模型分词算法"><a href="#1-2-1生成式模型分词算法" class="headerlink" title="1.2.1生成式模型分词算法"></a>1.2.1生成式模型分词算法</h3><p>生成式模型主要有n-gram模型、HMM隐马尔可夫模型、朴素贝叶斯分类等。在分词中应用比较多的是n-gram模型和HMM模型。</p>
<p>HMM:</p>
<p>HMM模型认为在解决序列标注问题时存在两种序列，一种是观测序列，即人们显性观察到的句子，而序列标签是隐状态序列，即观测序列为X，隐状态序列是Y，因果关系为Y-&gt;X。因此要得到标注结果Y，必须对X的概率、Y的概率、P(X|Y)进行计算，即建立P(X,Y)的概率分布模型。</p>
<p>HMM模型是常用的分词模型，基于Python的jieba分词器和基于Java的HanLP分词器都使用了HMM。要注意的是，该模型创建的概率图与上文中的DAG图并不同，因为节点具有观测概率，所以不能再用上文中的算法求解，而应该使用Viterbi算法求解最大概率的路径。</p>
<h3 id="1-2-2判别式模型分词算法"><a href="#1-2-2判别式模型分词算法" class="headerlink" title="1.2.2判别式模型分词算法"></a>1.2.2判别式模型分词算法</h3><p>判别式模型主要有感知机、SVM支持向量机、CRF条件随机场、最大熵模型等。在分词中常用的有感知机模型和CRF模型：</p>
<ol>
<li>平均感知机分词算法</li>
</ol>
<p>感知机是一种简单的二分类线性模型，通过构造超平面，将特征空间（输入空间）中的样本分为正负两类。通过组合，感知机也可以处理多分类问题。但由于每次迭代都会更新模型的所有权重，被误分类的样本会造成很大影响，因此采用平均的方法，在处理完一部分样本后对更新的权重进行平均。</p>
<ol start="2">
<li>CRF分词算法</li>
</ol>
<p>CRF可以看作一个无向图模型，对于给定的标注序列Y和观测序列X，对条件概率P(Y|X)进行定义，而不是对联合概率建模。CRF可以说是目前最常用的分词、词性标注和实体识别算法，它对未登陆词有很好的识别能力，但开销较大。</p>
<h3 id="1-2-3-神经网络分词算法"><a href="#1-2-3-神经网络分词算法" class="headerlink" title="1.2.3 神经网络分词算法"></a>1.2.3 神经网络分词算法</h3><p>在NLP中，最常用的神经网络为循环神经网络（RNN，Recurrent Neural Network），它在处理变长输入和序列输入问题中有着巨大的优势。LSTM为RNN变种的一种，在一定程度上解决了RNN在训练过程中梯度消失和梯度爆炸的问题。双向（Bidirectional）循环神经网络分别从句子的开头和结尾开始对输入进行处理，将上下文信息进行编码，提升预测效果。</p>
<p>目前对于序列标注任务，公认效果最好的模型是BiLSTM+CRF。结构如图：</p>
<p><img data-src="https://uploader.shimo.im/f/I8kRaKVkttcnYUun.png!thumbnail?fileGuid=3G3Pj63XGVtyqRcd" alt="图片"></p>
<p>利用双向循环神经网络BiLSTM，相比于上述其它模型，可以更好的编码当前字等上下文信息，并在最终增加CRF层，核心是用Viterbi算法进行解码，以得到全局最优解，避免B,S,E这种标记结果的出现。</p>
<h1 id="开源工具分析"><a href="#开源工具分析" class="headerlink" title="开源工具分析"></a>开源工具分析</h1><h2 id="公开数据源测试结果"><a href="#公开数据源测试结果" class="headerlink" title="公开数据源测试结果"></a>公开数据源测试结果</h2><p>细领域训练及测试结果</p>
<p>以下是在不同数据集上的对比结果：</p>
<table>
<thead>
<tr>
<th align="left">Default</th>
<th align="left">MSRA</th>
<th align="left">CTB8</th>
<th align="left">PKU</th>
<th align="left">WEIBO</th>
<th align="left">All Average</th>
</tr>
</thead>
<tbody><tr>
<td align="left">jieba</td>
<td align="left">81.45</td>
<td align="left">79.58</td>
<td align="left">81.83</td>
<td align="left">83.56</td>
<td align="left">81.61</td>
</tr>
<tr>
<td align="left">THULAC</td>
<td align="left">85.55</td>
<td align="left">87.84</td>
<td align="left">92.29</td>
<td align="left">86.65</td>
<td align="left">88.08</td>
</tr>
<tr>
<td align="left">pkuseg</td>
<td align="left">87.29</td>
<td align="left">91.77</td>
<td align="left">92.68</td>
<td align="left">93.43</td>
<td align="left">91.29</td>
</tr>
</tbody></table>
<h2 id="数据集介绍："><a href="#数据集介绍：" class="headerlink" title="数据集介绍："></a>数据集介绍：</h2><table>
<thead>
<tr>
<th align="left">语料库-英文</th>
<th align="left">语料库-中文</th>
<th align="left">语料内容</th>
<th align="left">语料语言</th>
<th align="left">来源</th>
<th align="left"></th>
<th align="left"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Academia Sinica</td>
<td align="left">中国科学院</td>
<td align="left"></td>
<td align="left">繁体</td>
<td align="left">第二届国际汉语分词大赛-2005年</td>
<td align="left"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/as_spec.doc?fileGuid=3G3Pj63XGVtyqRcd">516 KB</a></td>
<td align="left"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/as_spec.pdf?fileGuid=3G3Pj63XGVtyqRcd">336 KB</a></td>
</tr>
<tr>
<td align="left">City University of Hong Kong</td>
<td align="left">香港城市大学</td>
<td align="left"></td>
<td align="left">繁体</td>
<td align="left">第二届国际汉语分词大赛-2005年</td>
<td align="left"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/cityu_spec.doc?fileGuid=3G3Pj63XGVtyqRcd">154 KB</a></td>
<td align="left"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/cityu_spec.pdf?fileGuid=3G3Pj63XGVtyqRcd">237 KB</a></td>
</tr>
<tr>
<td align="left">Peking University(PKU)</td>
<td align="left">北京大学</td>
<td align="left"></td>
<td align="left">简体</td>
<td align="left">第二届国际汉语分词大赛-2005年</td>
<td align="left"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/pku_spec.doc?fileGuid=3G3Pj63XGVtyqRcd">177 KB</a></td>
<td align="left"><a href="http://sighan.cs.uchicago.edu/bakeoff2005/data/pku_spec.pdf?fileGuid=3G3Pj63XGVtyqRcd">294 KB</a></td>
</tr>
<tr>
<td align="left">Microsoft Researc(MSRA)</td>
<td align="left">微软亚洲研究院</td>
<td align="left"></td>
<td align="left">简体</td>
<td align="left">第二届国际汉语分词大赛-2005年</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">CTB8</td>
<td align="left"></td>
<td align="left">新闻文本及网络文本的混合型语料</td>
<td align="left"></td>
<td align="left">LDC</td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr>
<td align="left">WEIBO</td>
<td align="left">微博</td>
<td align="left">微博（网络文本语料）</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="left">数据集</th>
<th align="left">实例数（行数）</th>
<th align="left">Token数（词数）</th>
<th align="left">字节数（B）</th>
</tr>
</thead>
<tbody><tr>
<td align="left">pku-train</td>
<td align="left">292,541</td>
<td align="left">6,902,429</td>
<td align="left">39M</td>
</tr>
<tr>
<td align="left">pku-holdout</td>
<td align="left">5,000</td>
<td align="left">118,852</td>
<td align="left">685K</td>
</tr>
<tr>
<td align="left">pku-test</td>
<td align="left">7,500</td>
<td align="left">177,058</td>
<td align="left">1017K</td>
</tr>
<tr>
<td align="left">weibo-train</td>
<td align="left">49,057</td>
<td align="left">895,100</td>
<td align="left">4.7M</td>
</tr>
<tr>
<td align="left">weibo-holdout</td>
<td align="left">3,000</td>
<td align="left">59,940</td>
<td align="left">318K</td>
</tr>
<tr>
<td align="left">weibo-test</td>
<td align="left">5,000</td>
<td align="left">98,382</td>
<td align="left">524K</td>
</tr>
</tbody></table>
<p><a href="http://sighan.cs.uchicago.edu/bakeoff2005/?fileGuid=3G3Pj63XGVtyqRcd">数据地址： http://sighan.cs.uchicago.edu/bakeoff2005/</a></p>
<p>CTB8：新闻文本及网络文本的混合型语料</p>
<p>WEIBO：微博（网络文本语料）</p>
<p>MSRA 数据由第二届国际汉语分词评测比赛提供，CTB8 数据由 LDC 提供，WEIBO 数据由 NLPCC 分词比赛提供。在 GitHub 项目中，这三个预训练模型都提供了下载地址。</p>
<h2 id="pkuseg（最近更新时间-2019年10月）"><a href="#pkuseg（最近更新时间-2019年10月）" class="headerlink" title="pkuseg（最近更新时间-2019年10月）"></a>pkuseg（最近更新时间-2019年10月）</h2><p>pkuseg 是基于论文[Luo et. al, 2019]的工具包。其简单易用，支持细分领域分词(实际上只有，有效提升了分词准确度。</p>
<p>项目地址：<a href="https://github.com/lancopku/PKUSeg-python?fileGuid=3G3Pj63XGVtyqRcd">https://github.com/lancopku/PKUSeg-python</a></p>
<p>论文地址:<a href="https://arxiv.org/abs/1906.11455?fileGuid=3G3Pj63XGVtyqRcd">https://arxiv.org/abs/1906.11455</a></p>
<p>已有的预训练模型（现有的细分领域如下）</p>
<p>|<ul><li>news: 在MSRA（新闻语料）上训练的模型。</li></ul>|<br>|:—-|:—-|<br>|<ul><li>web: 在微博（网络文本语料）上训练的模型。</li></ul>|<br>|<ul><li>medicine: 在医药领域上训练的模型。</li></ul>|<br>|<ul><li>tourism: 在旅游领域上训练的模型。</li></ul>|<br>|<ul><li>mixed: 混合数据集训练的通用模型。随pip包附带的是此模型。</li></ul>|</p>
<h3 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h3><p>pkuseg具有如下几个特点（来自pkuseg项目文档）：</p>
<ol>
<li>多领域分词。</li>
</ol>
<p>不同于以往的通用中文分词工具，此工具包同时致力于为不同领域的数据提供个性化的预训练模型。根据待分词文本的领域特点，用户可以自由地选择不同的模型。 我们目前支持了新闻领域，网络领域，医药领域，旅游领域，以及混合领域的分词预训练模型。在使用中，如果用户明确待分词的领域，可加载对应的模型进行分词。如果用户无法确定具体领域，推荐使用在混合领域上训练的通用模型。</p>
<ol start="2">
<li>更高的分词准确率。</li>
</ol>
<p>相比于其他的分词工具包，当使用相同的训练数据和测试数据，pkuseg可以取得更高的分词准确率。</p>
<ol start="3">
<li>支持用户自训练模型。</li>
</ol>
<p>支持用户使用全新的标注数据进行训练。</p>
<ol start="4">
<li>支持词性标注。<h3 id="编译和安装"><a href="#编译和安装" class="headerlink" title="编译和安装"></a>编译和安装</h3></li>
</ol>
<ul>
<li>安装环境要求：目前仅支持python3</li>
<li>为了获得好的效果和速度，强烈建议大家通过pip install更新到目前的最新版本</li>
</ul>
<p>安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、pip3 install pkuseg</span><br></pre></td></tr></table></figure>
<h2 id="hanlp-持续更新，可根据领域自行训练）"><a href="#hanlp-持续更新，可根据领域自行训练）" class="headerlink" title="hanlp(持续更新，可根据领域自行训练）"></a>hanlp(持续更新，可根据领域自行训练）</h2><h2 id="fastHan"><a href="#fastHan" class="headerlink" title="fastHan"></a>fastHan</h2><h2 id="smoothnlp"><a href="#smoothnlp" class="headerlink" title="smoothnlp"></a>smoothnlp</h2><h2 id="jieba"><a href="#jieba" class="headerlink" title="jieba"></a>jieba</h2><h3 id="语料-共计594条："><a href="#语料-共计594条：" class="headerlink" title="语料-共计594条："></a>语料-共计594条：</h3><h4 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h4><p>1、富国上证指数ETF联接：2020年第4季度报告</p>
<p>2、华泰柏瑞蝉联 “十佳ETF管理人“ 旗下多只ETF过百亿 总规模超800亿元<a href="http://fund.jrj.com.cn/2021/01/25142231764732.shtml?fileGuid=3G3Pj63XGVtyqRcd">http://fund.jrj.com.cn/2021/01/25142231764732.shtml</a></p>
<p>3、贝莱德解读：疫情第二年，如何投资全球医疗保健板块？</p>
<p><a href="http://fund.jrj.com.cn/2021/01/22162231745207.shtml?fileGuid=3G3Pj63XGVtyqRcd">http://fund.jrj.com.cn/2021/01/22162231745207.shtml</a></p>
<p>4、浦发银行天添盈增利1号净值型理财计划</p>
<p>5、知乎搜索金融专业术语和文章</p>
<h4 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h4><p>将文本文档按中文句号切分得到单个句子</p>
<h3 id="难点："><a href="#难点：" class="headerlink" title="难点："></a>难点：</h3><p>分词颗粒度问题</p>
<h3 id="分析方法："><a href="#分析方法：" class="headerlink" title="分析方法："></a>分析方法：</h3><p>先用一中分词方法进行分词，查看是否符合，对其进行纠正，以其为标准答案；其他分词与答案不对的进行分析，看哪个方法合理</p>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
</search>
